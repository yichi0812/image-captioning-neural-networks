{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Image Captioning with Deep Learning - Training Notebook\n",
    "\n",
    "This notebook trains both RNN and Transformer models for automated image captioning.\n",
    "\n",
    "**Expected Training Time:** ~2-3 hours with free Tesla T4 GPU\n",
    "\n",
    "## Quick Start\n",
    "1. Click **Runtime** â†’ **Change runtime type** â†’ Select **T4 GPU**\n",
    "2. Click **Runtime** â†’ **Run all** (or press Ctrl+F9)\n",
    "3. Wait for training to complete\n",
    "4. Download trained models from the Files panel\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected! Please enable GPU: Runtime â†’ Change runtime type â†’ T4 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers pillow tqdm nltk matplotlib seaborn gradio\n",
    "!pip install -q git+https://github.com/jmhessel/pycocoevalcap.git\n",
    "\n",
    "print(\"âœ… All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clone Repository and Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/yichi0812/image-captioning-neural-networks.git\n",
    "%cd image-captioning-neural-networks\n",
    "\n",
    "print(\"âœ… Repository cloned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Flickr8k dataset from Kaggle\n",
    "# You need to upload your kaggle.json file first\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"ðŸ“¤ Please upload your kaggle.json file (from https://www.kaggle.com/settings/account)\")\n",
    "print(\"   If you don't have it: Go to Kaggle â†’ Account â†’ Create New API Token\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup Kaggle credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "print(\"âœ… Kaggle credentials configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Flickr8k dataset\n",
    "!pip install -q kaggle\n",
    "!kaggle datasets download -d adityajn105/flickr8k\n",
    "!unzip -q flickr8k.zip -d data/\n",
    "!mv data/Images data/images\n",
    "\n",
    "print(\"âœ… Dataset downloaded and extracted!\")\n",
    "print(f\"Total images: {len(os.listdir('data/images'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/val/test splits\n",
    "!python src/prepare_data.py --dataset flickr8k --data-dir ./data\n",
    "\n",
    "print(\"âœ… Dataset preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train RNN Model\n",
    "\n",
    "Training RNN with attention mechanism (~1 hour on T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train RNN model\n",
    "!python src/train.py --model rnn --epochs 20 --batch-size 64\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"\\nâœ… RNN training complete! Time: {elapsed:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Train Transformer Model\n",
    "\n",
    "Training Transformer decoder (~1.5 hours on T4 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Train Transformer model\n",
    "!python src/train.py --model transformer --epochs 20 --batch-size 64\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f\"\\nâœ… Transformer training complete! Time: {elapsed:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models and compare\n",
    "!python src/run_evaluation.py\n",
    "\n",
    "# Display results\n",
    "import json\n",
    "with open('outputs/evaluation_results.json', 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    for metric, score in metrics.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Generate Sample Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test caption generation on sample images\n",
    "import os\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get random test images\n",
    "test_images = random.sample(os.listdir('data/images'), 5)\n",
    "\n",
    "print(\"Sample Caption Generation:\\n\")\n",
    "for img_name in test_images:\n",
    "    img_path = f'data/images/{img_name}'\n",
    "    \n",
    "    # Generate caption with RNN\n",
    "    !python src/generate_caption.py --image {img_path} --model rnn --checkpoint models/rnn/checkpoint_best.pth > /tmp/rnn_caption.txt\n",
    "    with open('/tmp/rnn_caption.txt', 'r') as f:\n",
    "        rnn_caption = f.read().strip()\n",
    "    \n",
    "    # Generate caption with Transformer\n",
    "    !python src/generate_caption.py --image {img_path} --model transformer --checkpoint models/transformer/checkpoint_best.pth > /tmp/transformer_caption.txt\n",
    "    with open('/tmp/transformer_caption.txt', 'r') as f:\n",
    "        transformer_caption = f.read().strip()\n",
    "    \n",
    "    # Display\n",
    "    img = Image.open(img_path)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"RNN: {rnn_caption}\\nTransformer: {transformer_caption}\", fontsize=10)\n",
    "    plt.show()\n",
    "    print(f\"Image: {img_name}\")\n",
    "    print(f\"  RNN: {rnn_caption}\")\n",
    "    print(f\"  Transformer: {transformer_caption}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate attention visualization for a sample image\n",
    "sample_image = test_images[0]\n",
    "!python src/visualize_attention.py \\\n",
    "    --image data/images/{sample_image} \\\n",
    "    --model rnn \\\n",
    "    --checkpoint models/rnn/checkpoint_best.pth \\\n",
    "    --output outputs/attention_viz.png\n",
    "\n",
    "# Display attention visualization\n",
    "from IPython.display import Image as IPImage\n",
    "display(IPImage('outputs/attention_viz.png'))\n",
    "print(\"âœ… Attention visualization generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Download Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the trained models\n",
    "!zip -r trained_models.zip models/ outputs/\n",
    "\n",
    "# Download\n",
    "from google.colab import files\n",
    "files.download('trained_models.zip')\n",
    "\n",
    "print(\"âœ… Models zipped and ready for download!\")\n",
    "print(\"\\nThe zip file contains:\")\n",
    "print(\"  - models/rnn/checkpoint_best.pth\")\n",
    "print(\"  - models/transformer/checkpoint_best.pth\")\n",
    "print(\"  - outputs/evaluation_results.json\")\n",
    "print(\"  - outputs/training.log\")\n",
    "print(\"  - outputs/attention_viz.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Launch Interactive Demo (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Gradio demo\n",
    "!python src/demo.py \\\n",
    "    --rnn-checkpoint models/rnn/checkpoint_best.pth \\\n",
    "    --transformer-checkpoint models/transformer/checkpoint_best.pth \\\n",
    "    --share\n",
    "\n",
    "print(\"\\nâœ… Demo launched! Click the public URL above to access the interactive demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "You now have:\n",
    "- âœ… Trained RNN model with attention\n",
    "- âœ… Trained Transformer model\n",
    "- âœ… Evaluation metrics (BLEU, METEOR, CIDEr, ROUGE)\n",
    "- âœ… Attention visualizations\n",
    "- âœ… Sample captions\n",
    "- âœ… Interactive demo\n",
    "\n",
    "### Next Steps\n",
    "1. Download the `trained_models.zip` file\n",
    "2. Extract it locally\n",
    "3. Use the models for your project report\n",
    "4. Run the demo locally: `python src/demo.py`\n",
    "\n",
    "### For Your Report\n",
    "- Training logs: `outputs/training.log`\n",
    "- Evaluation results: `outputs/evaluation_results.json`\n",
    "- Attention visualizations: `outputs/attention_viz.png`\n",
    "- Model checkpoints: `models/*/checkpoint_best.pth`\n",
    "\n",
    "---\n",
    "\n",
    "**Repository:** https://github.com/yichi0812/image-captioning-neural-networks\n",
    "\n",
    "**Total Training Time:** ~2-3 hours on free T4 GPU\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

